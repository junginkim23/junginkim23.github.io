---
title : "[Deep Learning] Activation Function"
excerpt: "활성화 함수에 대해 알아보자"
header:
    teaser: /assets/datascience.png
# author_profile: true
# sidebar:
#   nav: "sidebar-contents"
category :
    - DL 
tag : 
    - Deep Learning 
    - Activation function 
toc : true 
toc_sticky: true

last_modified_at: 2021-06-15T13:00-16:00
---

## 신경망 

- 입력층, 은닉층, 출력층으로 구성되어 있다. 

- 활성화 함수로 시그모이드 함수 사용 

## 퍼셉트론 

- w, b 매개변수가 존재. 
    - b는 편향이라고 하며, 얼마나 쉽게 뉴런이 활성화되느냐를 제어한다.

- 활성화 함수로 계단 함수 사용 

## 활성화 함수 

- 일반적으로 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 **활성화 함수**라고 한다. 

**시그모이드 & 계단 함수**의 차이? 
: 매끄러움이 가장 큰 차이로 볼 수 있다. 
: 퍼셉트론에서는 활성화 함수로 계단함수를 사용하여 입력 신호의 총합을 출력 신호 0 or 1로 바꿔서 보내주는 대신, 신경망에서는 활성화 함수로 시그모이드 함수를 사용하여 입력 신호의 총합을 연속적인 실수값으로 출력해준다.
: 두 함수의 공통점으로는 입력이 작을 때의 출력은 0에 가깝고 입력이 커지면 출력이 1에 가까워지는 구조가 된다. 그뿐만 아니라, 입력이 아무리 작거나 커도 출력은 0에서 1 사이의 값이 된다. 

- 신경망에서 활성화 함수로는 비선형 함수를 사용해야 한다. 그 이유는 선형 함수를 사용하게 되면 층을 깊게 하는 것의 의미가 없기 때문!

```py
h(x) = cx 이 활성화 함수를 사용한 3층 네트워크를 생각해보자. 
y(x) = h(h(h(x)))
     = c*c*c*x = ax (a ^ 3)
```

이처럼 선형함수를 이용해서 여러 층으로 구성하는 이점을 살릴 수 없다. 그래서 층을 쌓는 혜택을 얻고 싶다면 활성화 함수로 비선형 함수를 사용해야 한다.

- ReLu 함수

```py
def Relu(x):
    return np.maximum(0,x) # 0,x중 큰 값을 반환해주는 함수 maximum
```
