---
title : "[Deep Learning] 항등 함수 & 소프트 맥스 함수"
excerpt: "출력층에서의 항등 함수와 소프트 맥스 함수에 대해 알아보자."
header:
    teaser: /assets/datascience.png
# author_profile: true
# sidebar:
#   nav: "sidebar-contents"
category :
    - DL 
tag : 
    - Deep Learning 
    - softmax
    - identity function 
toc : true 
toc_sticky: true

last_modified_at: 2021-06-15T13:00-16:00
---

# 개요 

3층 신경망을 numpy를 이용해 구현해보았다. 구현한 3층 신경망의 출력을 위해 항등 함수를 활성화 함수로 사용했다. 신경망을 분류와 회귀 모두에서 사용할 수 있고, 각 신경망의 출력층에서 사용하는 활성화 함수 또한 다르다. 주로 **회귀에는 항등 함수**를 **분류에는 소프트 맥스 함수**를 사용한다. 

**분류 & 회귀**
- 분류 
분류란, 사진 속 인물의 성별을 분류하는 문제가 여기에 속한다. 

- 회귀
회귀란, 입력 데이터에서 연속적인 수치를 예측하는 문제이다.

## 소프트맥스 함수 구현 

```py
import numpy as np

# 입력 신호 a 
a = np.array([0.3,2.9,4.0]) 

def softmax(a):

    # 분자
    exp_a = np.exp(a)

    # 분모
    sum_exp_a = np.sum(exp_a)

    # 결과
    y = exp_a / sum_exp_a

return y 
```

> 위의 소프트 맥스 함수를 조금 수정할 필요가 있다. 그 이유는 오버플로우 때문이다. 오버 플로우란 컴퓨터로 표현할 수 있는 한정된 범위를 넘어서게 되면 에러가 발생하는데 이를 막기 위해서는 특정 조치가 필요하고 일반적으로는 입력 신호 중 최댓값을 뺌으로써 이러한 문제를 해결할 수 있다. 

> 소프트 맥스에서 큰 수치가 나와 오버플로우 문제가 발생하는 근본적인 이유는 지수함수(exp)때문이다.

## 수정된 소프트맥스 함수 

```py 

def softmax(a):
    # 입력 신호 중 최댓값
    c = np.max(a)

    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a 
    return y 

print(softmax([0.3,2.9,4.0])) # [0.01821127 0.24519181 0.73659691]
print(np.sum(softmax([0.3,2.9,4.0]))) # 1.0 

softmax 함수의 output은 항상 0보다 크고 1보다 작다.
또한, 총합은 항상 1이 된다는 특징이 있다.
```