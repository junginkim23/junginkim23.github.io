---
title : "[Deep Learning] neural network"
excerpt: "신경망에 대해 알아보자"
header:
    teaser: /assets/datascience.png
# author_profile: true
# sidebar:
#   nav: "sidebar-contents"
category :
    - DL 
tag : 
    - Deep Learning 
    - neural network 
toc : true 
toc_sticky: true

last_modified_at: 2021-06-15T13:00-16:00
---

# 개요

앞 절에서 퍼셉트론과 활성화 함수의 여러 종류에 대해서 학습했다. 지금부터는 신경망에 대해 학습해보겠다. 

# 3층 신경망 구현

- 신경망의 순 방향 구현 

```py
import numpy as np 

# 활성화 함수 
def sigmoid(x):
	return 1 / (1 + np.exp(-x))


def init_network():
	network  = {}
	# W1 (2,3) - 0.1, 0.3, 0.5 / 0.2, 0.4, 0.6
	network['W1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
	# B1 (3,) - 0.1, 0.2, 0.3
	network['b1'] = np.array([0.1, 0.2, 0.3]) 
	# W2 (3,2) - 0.1, 0.4 / 0.2, 0.5 / 0.3, 0.6 
	network['W2'] = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]])
	# B2 (2,) - 0.1, 0.2 
	network['b2'] = np.array([0.1,0.2])
	# W3 (2,2) - 0.1, 0.3 / 0.2, 0.4 
	network['W3'] = np.array([[0.1,0.3], [0.2,0.4]])
	# B3 (2,) - 0.1, 0.2 
	network['b3'] = np.array([0.1, 0.2])

	return network
# 마지막 layer에서 결과값을 내는 항등함수 
def identity_function(x):
	return x 

# 입력 신호를 출력 신호로 변환하는 함수
def forward(network, x): 
	# 가중치
	W1, W2, W3 = network['W1'], network['W2'], network['W3']
	b1, b2, b3 = network['b1'],network['b2'],network['b3']
	
	# 각 layer에서 입력 신호의 총합
	a1 = np.dot(x,W1) + b1 
	z1 = sigmoid(a1)	
	a2 = np.dot(z1,W2) + b2 
	z2 = sigmoid(a2)
	a3 = np.dot(z2,W3) + b3		
	
	y = identity_function(a3) 
	
	return y 

network = init_network()
x = np.array([1.0,0.5])
y = forward(network,x) 

print(y)
#output : [0.31682708 0.69627909]
```


**출력층에서 사용하는 활성화 함수는?**
일반적으로 신경망은 분류와 회귀에 모두 사용이 가능하다. 다만, 둘 중 어떤 문제냐에 따라 출력층에서 사용하는 함수가 달라진다. 보통, 회귀에는 항등 함수를 분류에는 소프트 맥스 함수를 사용한다. 
